# Next-generation baseline configuration for heterogeneous speculative decoding experiments.
# - 100 target servers spanning four capability tiers
# - 300 drafts grouped in four capability buckets with distinct generation latencies
# - Rich connectivity matrix capturing affinity, fanout limits, and network jitter
# - Designed to exercise VIDUR lookups and future acceptance predictors

sim_time_ms: 90000                 # 90 seconds wall-clock to capture steady-state behaviour
burn_in_ms: 5000                   # Ignore first 5s while queues warm up
seed: 31415                        # Deterministic reproducibility
verbose: true
debug: false

execution_mode: speculative        # Enable speculative draft/verify flow
gamma: 4                            # Draft chunk size (tokens per speculation round)

# Conversation mix — used when generating synthetic traces or running without a replay file.
answer_length_mean: 540
answer_length_std: 160
answer_length_min: 120
answer_length_max: 1024
use_answer_distribution: true
prompt_length_min: 48
prompt_length_max: 640
prompt_scale_by_capability: true
mixed_batching: true               # Allow prefill/decode co-resident in a batch

router: semi_clairvoyant
router_params:
  d_choices: 4                     # Used by JSQ-family routers; ignored by semi-clairvoyant

scheduler:
  type: decode_first               # Baseline with decode-first bias for mixed batches
  max_prefills_per_batch: 2        # Allow limited prefills alongside decodes
  adaptive_window: true            # Dynamically widen/narrow batch window via observed latency
  prefill_chunk_tokens: 128        # Chunk long prompts to reduce head-of-line blocking
  two_lane: true                   # Two-lane scheduling with separate queues
  prefill_share: 0.35              # Reserve 35% server time for prefills when two_lane=true

performance_model:
  type: vidur
  vidur:
    table_path: data/vidur/lut_cluster_v1.parquet   # Placeholder; supply real LUT bundle
    default_latency_ms: 42.0
    neighbors: 5
    prefer_exact: false
    bootstrap_defaults: true

# Optional trace replay (comment out to run synthetic workload). Defaults are merged into entries.
trace_path: null
trace_defaults:
  slo_class: standard
  mode_hint: auto

workload:
  arrival: poisson
  rate_rps: 220.0                   # High-rate workload to exercise resource contention

auto_topology:
  targets:
    count: 100
    tiers:
      - name: apex
        ratio: 0.20                  # 20 H100/H200-class racks
        model: llama-70b
        gpu: H100
        weight: 1.5
        batch_window_ms: 3.5
        batch_size: 40
        prefill_latency_per_token: 0.26
        decode_latency_per_token: 0.94
        energy_per_token_mj: 0.018
      - name: core
        ratio: 0.30                  # 30 A100-class racks
        model: llama-34b
        gpu: A100
        weight: 1.15
        batch_window_ms: 4.5
        batch_size: 48
        prefill_latency_per_token: 0.38
        decode_latency_per_token: 1.25
        energy_per_token_mj: 0.022
      - name: regional
        ratio: 0.30                  # 30 L40-class regional POPs
        model: llama-13b
        gpu: L40
        weight: 0.85
        batch_window_ms: 6.0
        batch_size: 56
        prefill_latency_per_token: 0.68
        decode_latency_per_token: 2.05
        energy_per_token_mj: 0.031
      - name: edge
        ratio: 0.20                  # 20 A10-class edge deployments
        model: llama-7b
        gpu: A10
        weight: 0.55
        batch_window_ms: 7.5
        batch_size: 64
        prefill_latency_per_token: 0.92
        decode_latency_per_token: 2.85
        energy_per_token_mj: 0.044

  drafts:
    count: 300
    gens_ms_per_gamma:
      - [5, 14]                     # Bucket 0: datacenter-resident drafts
      - [15, 36]                    # Bucket 1: regional GPU pods
      - [40, 86]                    # Bucket 2: enterprise / high-end laptops
      - [90, 210]                   # Bucket 3: consumer mobile/edge
    capability_map:
      0: 3.4
      1: 2.1
      2: 1.2
      3: 0.65
    draft_bucket_labels: [dc_fast, regional_gpu, enterprise, mobile]
    reliability:
      dc_fast: 0.995
      regional_gpu: 0.985
      enterprise: 0.970
      mobile: 0.940

  connectivity:
    fanout_per_draft: 5
    fanout_override:
      enterprise: 4
      mobile: 3
    affinity_rules:
      dc_fast: [apex, core]
      regional_gpu: [apex, core, regional]
      enterprise: [core, regional]
      mobile: [regional, edge]
    net_ms_ranges:
      apex: [1.5, 4.0]
      core: [4.0, 9.0]
      regional: [12.0, 28.0]
      edge: [24.0, 55.0]
    acceptance_by_tier:
      "dc_fast":    {apex: 0.93, core: 0.91}
      "regional_gpu": {apex: 0.90, core: 0.88, regional: 0.82}
      "enterprise":  {core: 0.82, regional: 0.76}
      "mobile":      {regional: 0.69, edge: 0.61}
    link_jitter_pct: 0.20            # Uniform ±20% jitter applied to sampled latencies
    drop_rate:
      enterprise: 0.005
      mobile: 0.012

# Additional knobs for synthetic trace generation (used by simulator/trace/generate_trace.py)
synthetic_trace_defaults:
  request_prefix: simreq
  attach_device_tier: true
  slo_budget_ms:
    premium: 1800
    standard: 2400
    relaxed: 3200

# Acceptance predictor placeholder configuration (future integration point).
acceptance_model:
  type: fallback                     # When implemented, switch to "vidur_lut" or "specpp"
  fallback_rate: 0.78                # Used until the acceptance estimator is wired in
