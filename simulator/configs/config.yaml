# Multi-cluster baseline configuration for advanced speculative decoding experiments.
# - 30 targets organized into three clusters (10/5/15)
# - 300 drafts split evenly across clusters with distinct capability buckets
# - Per-cluster routers and connectivity rules to exercise hierarchical scheduling

sim_time_ms: 90000
burn_in_ms: 5000
seed: 31415
verbose: true
debug: false

execution_mode: speculative
gamma: 4

# Conversation mix (used when generating synthetic traces)
answer_length_mean: 520
answer_length_std: 150
answer_length_min: 128
answer_length_max: 960
use_answer_distribution: true
prompt_length_min: 40
prompt_length_max: 512
prompt_scale_by_capability: true
mixed_batching: true

think_time:
  enabled: true
  distribution: lognormal
  mean_ms: 1800
  cv: 0.6
  min_ms: 250

router: wjsq2
router_params:
  d_choices: 3

scheduler:
  type: decode_first
  max_prefills_per_batch: 2
  adaptive_window: true
  prefill_chunk_tokens: 96
  two_lane: true
  prefill_share: 0.30

performance_model:
  type: vidur
  vidur:
    table_path: data/vidur/lut_cluster_v1.parquet
    default_latency_ms: 38.0
    neighbors: 5
    prefer_exact: false
    bootstrap_defaults: true

trace_path: null
trace_defaults:
  slo_class: standard
  mode_hint: auto

workload:
  arrival: poisson
  rate_rps: 160.0

auto_topology:
  clusters:
    - name: cluster_a
      router: semi_clairvoyant
      targets:
        count: 10
        tiers:
          - name: a_apex
            count: 4
            model: llama-70b
            gpu: H100
            weight: 1.6
            batch_window_ms: 3.5
            batch_size: 28
            prefill_latency_per_token: 0.24
            decode_latency_per_token: 0.88
          - name: a_core
            count: 6
            model: llama-34b
            gpu: A100
            weight: 1.25
            batch_window_ms: 4.5
            batch_size: 36
            prefill_latency_per_token: 0.36
            decode_latency_per_token: 1.18
      drafts:
        count: 100
        gens_ms_per_gamma:
          - [4, 10]
          - [12, 28]
          - [32, 64]
          - [72, 150]
        capability_map:
          0: 3.8
          1: 2.6
          2: 1.5
          3: 0.9
        draft_bucket_labels: [dc_ultra, dc_high, dc_mid, dc_edge]
        reliability:
          dc_ultra: 0.997
          dc_high: 0.992
          dc_mid: 0.983
          dc_edge: 0.970
      connectivity:
        fanout_per_draft: 5
        fanout_override:
          dc_mid: 4
          dc_edge: 3
        affinity_rules:
          dc_ultra: [a_apex]
          dc_high: [a_apex, a_core]
          dc_mid: [a_core]
          dc_edge: [a_core]
        net_ms_ranges:
          a_apex: [1.5, 3.5]
          a_core: [3.0, 6.5]
        acceptance_by_tier:
          "0": {a_apex: 0.95}
          "1": {a_apex: 0.93, a_core: 0.90}
          "2": {a_core: 0.86}
          "3": {a_core: 0.80}
        link_jitter_pct: 0.12
        drop_rate:
          dc_edge: 0.004

    - name: cluster_b
      router: wjsq2
      router_params:
        d_choices: 2
      targets:
        count: 5
        tiers:
          - name: b_regional
            count: 3
            model: llama-13b
            gpu: L40
            weight: 0.9
            batch_window_ms: 5.5
            batch_size: 32
            prefill_latency_per_token: 0.62
            decode_latency_per_token: 1.95
          - name: b_edge
            count: 2
            model: llama-7b
            gpu: A10
            weight: 0.65
            batch_window_ms: 7.0
            batch_size: 40
            prefill_latency_per_token: 0.90
            decode_latency_per_token: 2.70
      drafts:
        count: 100
        gens_ms_per_gamma:
          - [15, 32]
          - [38, 80]
          - [90, 160]
        capability_map:
          0: 2.2
          1: 1.2
          2: 0.7
        draft_bucket_labels: [rg_gpu, rg_enterprise, rg_mobile]
        reliability:
          rg_gpu: 0.985
          rg_enterprise: 0.970
          rg_mobile: 0.945
      connectivity:
        fanout_per_draft: 4
        fanout_override:
          rg_mobile: 3
        affinity_rules:
          rg_gpu: [b_regional]
          rg_enterprise: [b_regional, b_edge]
          rg_mobile: [b_edge]
        net_ms_ranges:
          b_regional: [10.0, 22.0]
          b_edge: [20.0, 45.0]
        acceptance_by_tier:
          "0": {b_regional: 0.88}
          "1": {b_regional: 0.82, b_edge: 0.75}
          "2": {b_edge: 0.66}
        link_jitter_pct: 0.18
        drop_rate:
          rg_mobile: 0.010

    - name: cluster_c
      router: jiq
      targets:
        count: 15
        tiers:
          - name: c_hub
            count: 5
            model: llama-34b
            gpu: A100
            weight: 1.1
            batch_window_ms: 4.8
            batch_size: 40
            prefill_latency_per_token: 0.40
            decode_latency_per_token: 1.32
          - name: c_regional
            count: 6
            model: llama-13b
            gpu: L4
            weight: 0.85
            batch_window_ms: 6.2
            batch_size: 44
            prefill_latency_per_token: 0.74
            decode_latency_per_token: 2.20
          - name: c_edge
            count: 4
            model: llama-7b
            gpu: T4
            weight: 0.55
            batch_window_ms: 8.5
            batch_size: 48
            prefill_latency_per_token: 1.05
            decode_latency_per_token: 3.10
      drafts:
        count: 100
        gens_ms_per_gamma:
          - [20, 42]
          - [48, 110]
          - [120, 220]
        capability_map:
          0: 1.9
          1: 1.0
          2: 0.55
        draft_bucket_labels: [metro, suburban, remote]
        reliability:
          metro: 0.980
          suburban: 0.955
          remote: 0.925
      connectivity:
        fanout_per_draft: 4
        fanout_override:
          remote: 2
        affinity_rules:
          metro: [c_hub]
          suburban: [c_hub, c_regional]
          remote: [c_regional, c_edge]
        net_ms_ranges:
          c_hub: [6.0, 14.0]
          c_regional: [16.0, 32.0]
          c_edge: [28.0, 60.0]
        acceptance_by_tier:
          "0": {c_hub: 0.89}
          "1": {c_hub: 0.84, c_regional: 0.78}
          "2": {c_regional: 0.70, c_edge: 0.60}
        link_jitter_pct: 0.22
        drop_rate:
          remote: 0.015

# Synthetic trace helper defaults (used by simulator/trace/generate_trace.py)
synthetic_trace_defaults:
  request_prefix: simreq
  attach_device_tier: true
  slo_budget_ms:
    premium: 2000
    standard: 2600
    relaxed: 3600
  include_cluster_label: true

acceptance_model:
  type: fallback
  fallback_rate: 0.76
