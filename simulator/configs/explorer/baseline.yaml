sim_time_ms: 90000
seed: 123
verbose: false
debug: false

execution_mode: speculative
gamma: 4

prompt_length_min: 24
prompt_length_max: 512
prompt_scale_by_capability: true
answer_length_mean: 420
answer_length_std: 120
answer_length_min: 96
answer_length_max: 960
use_answer_distribution: true
mixed_batching: true

router: random
router_params:
  d_choices: 3
global_router: disabled

workload:
  arrival: poisson
  rate_rps: 140.0

think_time:
  enabled: true
  distribution: lognormal
  mean_ms: 1700
  cv: 0.6
  min_ms: 220

trace_defaults:
  slo_class: hetero_baseline
  target_p95_latency_ms: 260.0
  target_p99_latency_ms: 360.0
  target_ttft_ms: 140.0

performance_model:
  type: vidur
  vidur:
    table_path: null
    bootstrap_defaults: false
    realtime_enabled: true
    realtime_cache_dir: data/vidur/cache

scheduler:
  type: baseline
  pools:
    fast_prefill:
      targets:
        - dc_core_t00
    fast_decode:
      targets:
        - dc_core_t01
    regional_pool:
      clusters:
        - regional
    edge_pool:
      clusters:
        - edge
  prefill:
    pool: fast_prefill
    queue_policy: priority
    max_batch_requests: 6
    max_wait_ms: 1.0
    delayed_batch_ms: 0.35
    max_queue_depth: 16
    backpressure_wait_ms: 0.08
    dynamic_policy:
      enabled: true
      low_queue_depth: 1
      high_queue_depth: 6
      low_wait_ms: 0.20
      high_wait_ms: 1.50
      low_batch_requests: 2
      high_batch_requests: 6
      low_delay_ms: 0.20
      high_delay_ms: 0.60
  decode:
    pool: fast_decode
    queue_policy: priority
    max_batch_requests: 5
    max_wait_ms: 0.9
    delayed_batch_ms: 0.25
    max_queue_depth: 18
    backpressure_wait_ms: 0.1
    dynamic_policy:
      enabled: true
      low_queue_depth: 2
      high_queue_depth: 8
      low_wait_ms: 0.15
      high_wait_ms: 1.00
      low_batch_requests: 2
      high_batch_requests: 6
      low_delay_ms: 0.10
      high_delay_ms: 0.50
  kv:
    default_capacity_tokens: 240000
    max_utilization_pct: 82.0

auto_topology:
  clusters:
    - name: dc_core
      router: semi_clairvoyant
      router_params: {}
      targets:
        count: 2
        tiers:
          - name: dc_prefill
            count: 1
            model: qwen-72b-prefill
            gpu: H100
            weight: 1.8
            batch_window_ms: 0.28
            batch_size: 12
            prefill_latency_per_token: 0.30
            decode_latency_per_token: 1.00
            vidur:
              model_name: Qwen/Qwen-72B
              device: H100
              network_device: H100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          - name: dc_decode
            count: 1
            model: llama-2-70b-decode
            gpu: H100
            weight: 1.6
            batch_window_ms: 0.35
            batch_size: 16
            prefill_latency_per_token: 0.34
            decode_latency_per_token: 1.05
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: H100
              network_device: H100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      drafts:
        count: 40
        gens_ms_per_gamma:
          - [8, 14]
          - [16, 26]
        capability_map:
          0: 1.35
          1: 0.95
        draft_bucket_labels: [rack_ultra, rack_high]
        reliability:
          rack_ultra: 0.997
          rack_high: 0.992
        metadata_by_label:
          rack_ultra:
            hardware: A40
            model_name: internlm-20b
            vidur_profile:
              model_name: internlm/internlm-20b
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          rack_high:
            hardware: A40
            model_name: llama-2-7b
            vidur_profile:
              model_name: meta-llama/Llama-2-7b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      connectivity:
        fanout_per_draft: 5
        net_ms_ranges:
          dc_prefill: [1.2, 2.2]
          dc_decode: [1.8, 3.0]
        acceptance_by_tier:
          "0": {dc_prefill: 0.96, dc_decode: 0.94}
          "1": {dc_prefill: 0.92, dc_decode: 0.90}
        link_jitter_pct: 0.04
    - name: regional
      router: wjsq2
      router_params:
        d_choices: 2
      targets:
        count: 2
        tiers:
          - name: regional_qwen
            count: 1
            model: qwen-72b
            gpu: A100
            weight: 1.2
            batch_window_ms: 0.9
            batch_size: 18
            prefill_latency_per_token: 0.48
            decode_latency_per_token: 1.35
            vidur:
              model_name: Qwen/Qwen-72B
              device: A100
              network_device: A100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          - name: regional_llama
            count: 1
            model: llama-2-70b
            gpu: A100
            weight: 1.1
            batch_window_ms: 0.8
            batch_size: 20
            prefill_latency_per_token: 0.55
            decode_latency_per_token: 1.45
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: A100
              network_device: A100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      drafts:
        count: 35
        gens_ms_per_gamma:
          - [14, 22]
          - [24, 38]
          - [40, 62]
        capability_map:
          0: 0.85
          1: 0.6
          2: 0.4
        draft_bucket_labels: [regional_high, regional_mid, regional_low]
        reliability:
          regional_high: 0.989
          regional_mid: 0.982
          regional_low: 0.970
        metadata_by_label:
          regional_high:
            hardware: A40
            model_name: llama-10b-draft
            vidur_profile:
              model_name: meta-llama/Llama-2-13b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          regional_mid:
            hardware: A40
            model_name: llama-7b-draft
            vidur_profile:
              model_name: meta-llama/Llama-2-7b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          regional_low:
            hardware: A40
            model_name: phi-2
            vidur_profile:
              model_name: microsoft/phi-2
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      connectivity:
        fanout_per_draft: 4
        fanout_override:
          regional_low: 3
        net_ms_ranges:
          regional_mix: [6.0, 12.0]
        acceptance_by_tier:
          "0": {regional_mix: 0.90}
          "1": {regional_mix: 0.85}
          "2": {regional_mix: 0.78}
        link_jitter_pct: 0.10
        drop_rate:
          regional_low: 0.006
    - name: edge
      router: random
      router_params: {}
      targets:
        count: 2
        tiers:
          - name: edge_heavy
            count: 1
            model: llama-2-70b
            gpu: A40
            weight: 0.75
            batch_window_ms: 1.8
            batch_size: 10
            prefill_latency_per_token: 1.10
            decode_latency_per_token: 2.70
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          - name: edge_mid
            count: 1
            model: codellama-34b
            gpu: A40
            weight: 0.65
            batch_window_ms: 1.6
            batch_size: 12
            prefill_latency_per_token: 0.92
            decode_latency_per_token: 2.35
            vidur:
              model_name: codellama/CodeLlama-34b-Instruct-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      drafts:
        count: 25
        gens_ms_per_gamma:
          - [24, 42]
          - [48, 82]
        capability_map:
          0: 0.34
          1: 0.22
        draft_bucket_labels: [edge_mid, edge_low]
        reliability:
          edge_mid: 0.960
          edge_low: 0.935
        metadata_by_label:
          edge_mid:
            hardware: A40
            model_name: codellama-34b
            vidur_profile:
              model_name: meta-llama/Llama-2-13b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          edge_low:
            hardware: A40
            model_name: phi-2
            vidur_profile:
              model_name: microsoft/phi-2
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      connectivity:
        fanout_per_draft: 3
        net_ms_ranges:
          edge_gpu: [18.0, 28.0]
          edge_cpu: [25.0, 38.0]
        acceptance_by_tier:
          "0": {edge_gpu: 0.70, edge_cpu: 0.62}
          "1": {edge_gpu: 0.60, edge_cpu: 0.55}
        link_jitter_pct: 0.18
        drop_rate:
          edge_low: 0.015
