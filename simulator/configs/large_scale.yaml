sim_time_ms: 120000
seed: 2025
verbose: false
router: wjsq2
router_params:
  d_choices: 4
mixed_batching: true
trace_path: trace/large_scale_trace.jsonl
trace_defaults:
  slo_class: standard
  mode_hint: auto

auto_topology:
  targets:
    count: 200
    tiers:
      - name: core
        ratio: 0.35
        model: llama-70b
        gpu: H100
        weight: 1.3
        batch_window_ms: 4.0
        batch_size: 32
        prefill_latency_per_token: 0.30
        decode_latency_per_token: 1.20
      - name: regional
        ratio: 0.30
        model: llama-34b
        gpu: A100
        weight: 1.0
        batch_window_ms: 5.0
        batch_size: 40
        prefill_latency_per_token: 0.45
        decode_latency_per_token: 1.60
      - name: edge
        ratio: 0.20
        model: llama-13b
        gpu: L40
        weight: 0.8
        batch_window_ms: 6.0
        batch_size: 48
        prefill_latency_per_token: 0.70
        decode_latency_per_token: 2.10
      - name: satellite
        ratio: 0.15
        model: llama-7b
        gpu: A10
        weight: 0.6
        batch_window_ms: 8.0
        batch_size: 64
        prefill_latency_per_token: 0.95
        decode_latency_per_token: 2.60

  drafts:
    count: 800
    gens_ms_per_gamma:
      - [6, 18]
      - [20, 45]
      - [60, 110]
      - [140, 260]
    capability_map:
      0: 3.0
      1: 1.8
      2: 1.1
      3: 0.6
    draft_bucket_labels: [datacenter, regional, edge, mobile]

  connectivity:
    fanout_per_draft: 4
    fanout_override:
      edge: 3
      mobile: 2
    affinity_rules:
      datacenter: [core, regional]
      regional: [core, regional]
      edge: [regional, edge]
      mobile: [edge, satellite]
    net_ms_ranges:
      core: [2, 6]
      regional: [6, 14]
      edge: [18, 40]
      satellite: [35, 70]
    acceptance_by_tier:
      "0": {core: 0.94, regional: 0.90}
      "1": {core: 0.88, regional: 0.85, edge: 0.78}
      "2": {regional: 0.80, edge: 0.72, satellite: 0.65}
      "3": {edge: 0.68, satellite: 0.60}

workload:
  arrival: poisson
  rate_rps: 120.0

answer_length_mean: 480
answer_length_std: 120
answer_length_min: 80
answer_length_max: 960
use_answer_distribution: true
prompt_length_min: 32
prompt_length_max: 400
prompt_scale_by_capability: true

performance_model:
  type: default

scheduler:
  type: baseline
