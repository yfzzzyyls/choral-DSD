# Trace-driven variant of baseline.yaml for GSM8K workloads
sim_time_ms: 10000
seed: 123
verbose: true
debug: true

speculation:
  framework: eagle
  execution_mode: fused
  acceptance_model: acceptance/llama2_7b_vs_70b.joblib
  gamma_policy:
    type: specpp
    min_gamma: 2
    max_gamma: 8
    stop_threshold: 0.7
    fallback_gamma: 4
  vanilla:
    # Vanilla-specific knobs go here (leave empty for defaults)
  eagle:
    beam_width: 4
    max_depth: 4
    min_depth: 1

execution_mode: blocking
gamma: 4

trace_path: traces/gsm8k_trace_10s.jsonl
trace_replay:
  per_draft_rps: 3.5
trace_defaults:
  slo_class: hetero_baseline

prompt_length_min: 24
prompt_length_max: 512
prompt_scale_by_capability: true
answer_length_mean: 420
answer_length_std: 120
answer_length_min: 96
answer_length_max: 960
use_answer_distribution: true
mixed_batching: true

router: semi_clairvoyant
router_params: {}
global_router: disabled

workload:
  arrival: poisson
  rate_rps: 140.0

think_time:
  enabled: true
  distribution: lognormal
  mean_ms: 1700
  cv: 0.6
  min_ms: 220

performance_model:
  type: vidur
  vidur:
    table_path: null
    bootstrap_defaults: false
    realtime_enabled: true
    realtime_cache_dir: data/vidur/cache

scheduler:
  type: baseline
  pools:
    global_pool:
      clusters:
        - llama
  prefill:
    pool: global_pool
    queue_policy: priority
    max_batch_requests: 6
    max_wait_ms: 0.8
    delayed_batch_ms: 0.3
    max_queue_depth: 24
    backpressure_wait_ms: 0.08
    dynamic_policy:
      enabled: true
      low_queue_depth: 1
      high_queue_depth: 6
      low_wait_ms: 0.2
      high_wait_ms: 1.4
      low_batch_requests: 2
      high_batch_requests: 6
      low_delay_ms: 0.18
      high_delay_ms: 0.55
  decode:
    pool: global_pool
    queue_policy: priority
    max_batch_requests: 6
    max_wait_ms: 0.8
    delayed_batch_ms: 0.28
    max_queue_depth: 28
    backpressure_wait_ms: 0.09
    dynamic_policy:
      enabled: true
      low_queue_depth: 2
      high_queue_depth: 8
      low_wait_ms: 0.18
      high_wait_ms: 0.95
      low_batch_requests: 2
      high_batch_requests: 6
      low_delay_ms: 0.10
      high_delay_ms: 0.50
  kv:
    default_capacity_tokens: 240000
    max_utilization_pct: 82.0

auto_topology:
  clusters:
    - name: llama
      router: semi_clairvoyant
      router_params: {}
      targets:
        count: 20
        tiers:
          - name: llama_a100
            count: 10
            model: llama-2-70b
            gpu: A100
            weight: 1.6
            batch_window_ms: 0.35
            batch_size: 12
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: A100
              network_device: A100_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          - name: llama_h100
            count: 10
            model: llama-2-70b
            gpu: H100
            weight: 1.9
            batch_window_ms: 0.3
            batch_size: 14
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: H100
              network_device: H100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      drafts:
        count: 40
        capability_map:
          0: 0.5
          1: 0.5
        draft_bucket_labels: [llama_a40_fast, llama_a40_slow]
        reliability:
          llama_a40_fast: 0.985
          llama_a40_slow: 0.965
        metadata_by_label:
          llama_a40_fast:
            hardware: A40
            model_name: llama-2-7b
            vidur_profile:
              model_name: meta-llama/Llama-2-7b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
          llama_a40_slow:
            hardware: A40
            model_name: llama-2-7b
            vidur_profile:
              model_name: meta-llama/Llama-2-7b-hf
              device: A40
              network_device: A40_PAIRWISE_NVLINK
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
              chunk_size: 512
      connectivity:
        fanout_per_draft: 4
        net_ms_ranges:
          llama_a100: [6.5, 11.0]
          llama_h100: [5.0, 9.0]
        link_jitter_pct: 0.12
        drop_rate:
          llama_a40_slow: 0.006
