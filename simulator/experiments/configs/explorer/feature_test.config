sim_time_ms: 60000
seed: 123
verbose: false
debug: false
execution_mode: blocking
gamma: 4

prompt_length_min: 10
prompt_length_max: 200
prompt_scale_by_capability: true
answer_length: 256
answer_length_mean: 256
answer_length_std: 64
answer_length_min: 64
answer_length_max: 640
use_answer_distribution: true

mixed_batching: false
router: random
global_router: disabled

workload:
  arrival: poisson
  rate_rps: 120.0

performance_model:
  type: vidur
  vidur:
    realtime_enabled: true
    realtime_cache_dir: data/vidur/cache

auto_topology:
  clusters:
    - name: datacenter
      id_prefix: dc
      router: random
      targets:
        count: 20
        tiers:
          - name: prefill
            count: 10
            model: prefill-h100
            gpu: H100
            weight: 1.6
            batch_window_ms: 0.0
            batch_size: 4
            prefill_latency_per_token: 0.25
            decode_latency_per_token: 1.0
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: H100
              network_device: H100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
          - name: decode
            count: 10
            model: decode-h100
            gpu: H100
            weight: 1.6
            batch_window_ms: 0.0
            batch_size: 4
            prefill_latency_per_token: 0.3
            decode_latency_per_token: 0.9
            vidur:
              model_name: meta-llama/Llama-2-70b-hf
              device: H100
              network_device: H100_DGX
              tensor_parallel: 1
              pipeline_parallel: 1
              scheduler: sarathi
      drafts:
        count: 200
        gens_ms_per_gamma:
          - [4.0, 6.0]
          - [6.5, 9.0]
          - [9.5, 13.0]
        draft_bucket_labels: [llama-1b, llama-3b, llama-7b]
        capability_map:
          llama-1b: 1.4
          llama-3b: 1.0
          llama-7b: 0.7
        default_capability: 1.0
        default_reliability: 0.99
        metadata_by_label:
          llama-1b:
            model: meta-llama/Llama-2-1b-hf
          llama-3b:
            model: meta-llama/Llama-2-3b-hf
          llama-7b:
            model: meta-llama/Llama-2-7b-hf
      connectivity:
        fanout_per_draft: 6
        network_model:
          type: clos
          spine_count: 4
          leaf_count: 6
          hop_latency_ms: 0.35
          device_edge_latency_ms: 0.12
          jitter_pct: 0.05

scheduler:
  pools:
    prefill_pool:
      targets:
        - dc_t00
        - dc_t01
        - dc_t02
        - dc_t03
        - dc_t04
        - dc_t05
        - dc_t06
        - dc_t07
        - dc_t08
        - dc_t09
    decode_pool:
      targets:
        - dc_t10
        - dc_t11
        - dc_t12
        - dc_t13
        - dc_t14
        - dc_t15
        - dc_t16
        - dc_t17
        - dc_t18
        - dc_t19
  prefill:
    pool: prefill_pool
    queue_policy: fifo
    max_batch_requests: 16
    max_wait_ms: 0.3
    chunk_tokens: 192
    max_batch_tokens: 512
    max_queue_depth: 32
    backpressure_wait_ms: 0.005
  decode:
    pool: decode_pool
    queue_policy: fifo
    mode: continuous
    chunk_tokens: 8
    max_batch_requests: 8
    max_batch_tokens: 128
    max_wait_ms: 0.4
    max_queue_depth: 32
    backpressure_wait_ms: 0.005
  kv:
    default_capacity_tokens: 200000
    max_utilization_pct: 85.0
