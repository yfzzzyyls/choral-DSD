# Copyright 2025
# SPDX-License-Identifier: Apache-2.0
"""
Acceptance-rate regressor baseline inspired by VIDUR's profiling pipeline.

This script consumes per-iteration speculative decoding logs (generated by
``speculative.py`` with ``--details-jsonl``) and trains two models:

1. A regressor that predicts the number of speculative tokens accepted in each
   iteration (i.e., accepted_count).
2. A classifier that predicts the probability a token at position `pos` is
   accepted given the context length and position index.

Both models are RandomForest ensembles (matching VIDUR's default baseline).
The trained models plus lightweight metadata are serialized with joblib.

Example:
    python simulator/experiments/speculative/regressor.py \
        --details-jsonl results/simple_details.jsonl \
        --output-model acceptance/llama2_7b_vs_70b.joblib \
        --spec-tokens 4 \
        --metadata drafter=meta-llama/Llama-2-7b-hf \
        --metadata verifier=meta-llama/Llama-2-70b-hf \
        --print-report
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
import sys
from typing import Dict, Iterable, List, Optional, Tuple

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split

from simulator.experiments.speculative.metrics import (
    classification_metrics,
    positive_class_probabilities,
    regression_metrics,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train acceptance-rate regressors from speculative logs.",
    )
    parser.add_argument(
        "--details-jsonl",
        type=Path,
        required=True,
        help="JSONL file produced by speculative.py (--details-jsonl).",
    )
    parser.add_argument(
        "--spec-tokens",
        type=int,
        required=True,
        help="Number of speculative positions (k).",
    )
    parser.add_argument(
        "--output-model",
        type=Path,
        required=True,
        help="Destination joblib file for the trained models.",
    )
    parser.add_argument(
        "--metadata",
        type=str,
        nargs="*",
        default=[],
        help="Optional key=value metadata to store alongside the model.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.2,
        help="Fraction of iterations/positions used for validation (default 0.2).",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=1337,
        help="Random seed for train/test split and model initialisation.",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Number of trees for RandomForest models (default 200).",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Maximum tree depth (default None => unrestricted).",
    )
    parser.add_argument(
        "--print-report",
        action="store_true",
        help="Print training/validation metrics after fitting.",
    )
    return parser.parse_args()


def load_records(path: Path) -> List[dict]:
    records: List[dict] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_number, line in enumerate(handle, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                payload = json.loads(line)
            except json.JSONDecodeError as exc:  # pragma: no cover
                raise ValueError(f"{path}:{line_number}: invalid JSON: {exc}") from exc
            records.append(payload)
    if not records:
        raise ValueError(f"No records found in {path}")
    return records


def parse_metadata(pairs: Iterable[str]) -> Dict[str, str]:
    metadata: Dict[str, str] = {}
    for pair in pairs:
        if "=" not in pair:
            raise ValueError(f"Metadata entry '{pair}' must be key=value")
        key, value = pair.split("=", 1)
        metadata[key] = value
    return metadata


def build_datasets(
    records: Iterable[dict],
    spec_tokens: int,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Returns:
        X_count (n_iterations, n_features),
        y_count (accepted_count),
        X_accept (n_position_attempts, n_features),
        y_accept (0/1 acceptance)
    """
    count_features: List[List[float]] = []
    count_targets: List[int] = []
    accept_features: List[List[float]] = []
    accept_targets: List[int] = []

    for record in records:
        prompt_idx = record.get("prompt_index", 0)
        for iteration in record.get("iterations", []):
            context_len = float(iteration.get("context_length_before", 0))
            accepted_flags: List[bool] = list(iteration.get("accepted_flags", []))
            draft_token_ids: List[int] = list(iteration.get("draft_token_ids", []))
            spec_len = min(spec_tokens, len(draft_token_ids))
            accepted_count = sum(1 for flag in accepted_flags if flag)

            count_features.append([context_len])
            count_targets.append(accepted_count)

            for pos in range(min(spec_len, spec_tokens)):
                if pos < len(accepted_flags):
                    flag = 1 if accepted_flags[pos] else 0
                    accept_features.append([context_len, float(pos)])
                    accept_targets.append(flag)

    if not count_features or not accept_features:
        raise ValueError("Insufficient data to train regressors.")

    X_count = np.array(count_features, dtype=np.float32)
    y_count = np.array(count_targets, dtype=np.float32)
    X_accept = np.array(accept_features, dtype=np.float32)
    y_accept = np.array(accept_targets, dtype=np.int32)

    return X_count, y_count, X_accept, y_accept


def train_models(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
) -> Tuple[
    RandomForestRegressor,
    RandomForestClassifier,
    Dict[str, float],
]:
    # Accepted-count regressor
    Xc_train, Xc_test, yc_train, yc_test = train_test_split(
        X_count,
        y_count,
        test_size=test_size,
        random_state=random_state,
    )
    reg = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    reg.fit(Xc_train, yc_train)
    yc_pred = reg.predict(Xc_test)
    reg_mse = mean_squared_error(yc_test, yc_pred)
    reg_diagnostics = regression_metrics(yc_test, yc_pred)

    # Acceptance classifier
    Xa_train, Xa_test, ya_train, ya_test = train_test_split(
        X_accept,
        y_accept,
        test_size=test_size,
        random_state=random_state,
    )
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    clf.fit(Xa_train, ya_train)
    ya_proba = clf.predict_proba(Xa_test)
    ya_prob = positive_class_probabilities(clf, ya_proba)
    if len(getattr(clf, "classes_", [])) == 1:
        ya_pred = np.full(len(Xa_test), clf.classes_[0], dtype=np.int32)
    else:
        ya_pred = (ya_prob >= 0.5).astype(np.int32)
    clf_acc = accuracy_score(ya_test, ya_pred)
    clf_diagnostics = classification_metrics(ya_test, ya_pred, ya_prob, ece_bins=10)

    metrics = {
        "count_regression": reg_diagnostics,
        "accept_classification": clf_diagnostics,
        "count_mse": float(reg_mse),
        "accept_accuracy": float(clf_acc),
        "train_iterations": int(len(X_count)),
        "train_positions": int(len(X_accept)),
    }
    return reg, clf, metrics


def main() -> None:
    args = parse_args()
    records = load_records(args.details_jsonl)
    metadata = parse_metadata(args.metadata)

    X_count, y_count, X_accept, y_accept = build_datasets(
        records,
        spec_tokens=args.spec_tokens,
    )

    reg_model, clf_model, metrics = train_models(
        X_count,
        y_count,
        X_accept,
        y_accept,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.random_state,
        test_size=args.test_size,
    )

    bundle = {
        "metadata": metadata,
        "spec_tokens": args.spec_tokens,
        "feature_columns": {
            "count": ["context_length"],
            "accept": ["context_length", "position"],
        },
        "regressor": reg_model,
        "classifier": clf_model,
        "metrics": metrics,
        "details_source": str(args.details_jsonl),
    }

    args.output_model.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(bundle, args.output_model)

    if args.print_report:
        print("Acceptance regressor training report")
        print(
            f"  iterations={metrics['train_iterations']} positions={metrics['train_positions']}"
        )
        count = metrics["count_regression"]
        print(
            "  Count regression: "
            f"MSE={count['mse']:.4f} MAE={count['mae']:.4f} "
            f"bias={count['bias']:.4f} p95_abs={count['p95_abs_error']:.4f}"
        )
        accept = metrics["accept_classification"]
        calib = accept.get("calibration")
        calib_str = f" ECE={calib['ece']:.4f}" if calib else ""
        print(
            "  Accept classifier: "
            f"accuracy={accept['accuracy']:.4f} precision={accept['precision']:.4f} "
            f"recall={accept['recall']:.4f} f1={accept['f1']:.4f} "
            f"brier={accept['brier']:.4f}{calib_str}"
        )


if __name__ == "__main__":
    main()
