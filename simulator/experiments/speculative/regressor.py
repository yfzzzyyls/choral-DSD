# Copyright 2025
# SPDX-License-Identifier: Apache-2.0
"""
Acceptance-rate regressor baseline inspired by VIDUR's profiling pipeline.

This script consumes per-iteration speculative decoding logs (generated by
``speculative.py`` with ``--details-jsonl``) and trains two models:

1. A regressor that predicts the number of speculative tokens accepted in each
   iteration (i.e., accepted_count).
2. A classifier that predicts the probability a token at position `pos` is
   accepted given the context length and position index.

Both models are RandomForest ensembles (matching VIDUR's default baseline).
The trained models plus lightweight metadata are serialized with joblib.

Example:
    python simulator/experiments/speculative/regressor.py \
        --details-jsonl results/simple_details.jsonl \
        --output-model acceptance/llama2_7b_vs_70b.joblib \
        --spec-tokens 4 \
        --metadata drafter=meta-llama/Llama-2-7b-hf \
        --metadata verifier=meta-llama/Llama-2-70b-hf \
        --print-report
"""

from __future__ import annotations

import argparse
import json
from collections import defaultdict
from pathlib import Path
import sys
from typing import Dict, Iterable, List, Mapping, Optional, Tuple

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split

from simulator.experiments.speculative.metrics import (
    classification_metrics,
    positive_class_probabilities,
    regression_metrics,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train acceptance-rate regressors from speculative logs.",
    )
    parser.add_argument(
        "--details-jsonl",
        type=Path,
        required=True,
        help="JSONL file produced by speculative.py (--details-jsonl).",
    )
    parser.add_argument(
        "--spec-tokens",
        type=int,
        required=True,
        help="Number of speculative positions (k).",
    )
    parser.add_argument(
        "--output-model",
        type=Path,
        required=True,
        help="Destination joblib file for the trained models.",
    )
    parser.add_argument(
        "--metadata",
        type=str,
        nargs="*",
        default=[],
        help="Optional key=value metadata to store alongside the model.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.2,
        help="Fraction of iterations/positions used for validation (default 0.2).",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=1337,
        help="Random seed for train/test split and model initialisation.",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Number of trees for RandomForest models (default 200).",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Maximum tree depth (default None => unrestricted).",
    )
    parser.add_argument(
        "--print-report",
        action="store_true",
        help="Print training/validation metrics after fitting.",
    )
    return parser.parse_args()


def load_records(path: Path) -> List[dict]:
    records: List[dict] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_number, line in enumerate(handle, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                payload = json.loads(line)
            except json.JSONDecodeError as exc:  # pragma: no cover
                raise ValueError(f"{path}:{line_number}: invalid JSON: {exc}") from exc
            records.append(payload)
    if not records:
        raise ValueError(f"No records found in {path}")
    return records


def parse_metadata(pairs: Iterable[str]) -> Dict[str, str]:
    metadata: Dict[str, str] = {}
    for pair in pairs:
        if "=" not in pair:
            raise ValueError(f"Metadata entry '{pair}' must be key=value")
        key, value = pair.split("=", 1)
        metadata[key] = value
    return metadata



COUNT_FEATURES = ["context_length", "spec_tokens", "drafter_model", "verifier_model"]
ACCEPT_FEATURES = COUNT_FEATURES + ["position"]
CATEGORICAL_FEATURES = {"drafter_model", "verifier_model"}
_UNKNOWN_CATEGORY = "__UNKNOWN__"

def build_datasets(
    records: Iterable[dict],
    spec_tokens: int,
    metadata: Optional[Mapping[str, str]] = None,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, Dict[str, int]]]:
    """
    Returns:
        X_count (n_iterations, n_features),
        y_count (accepted_count),
        X_accept (n_position_attempts, n_features),
        y_accept (0/1 acceptance),
        categorical_maps (feature -> category -> index)
    """
    count_features: List[List[float]] = []
    count_targets: List[int] = []
    accept_features: List[List[float]] = []
    accept_targets: List[int] = []
    category_maps: Dict[str, Dict[str, int]] = defaultdict(dict)
    metadata = metadata or {}

    def encode_category(feature: str, value: Optional[str]) -> float:
        key = str(value) if value is not None else _UNKNOWN_CATEGORY
        mapping = category_maps[feature]
        if key not in mapping:
            mapping[key] = len(mapping)
        return float(mapping[key])

    def feature_value(
        name: str,
        base: Dict[str, object],
        position: Optional[int],
    ) -> float:
        if name == "context_length":
            return float(base.get("context_length", 0.0))
        if name == "position":
            return float(position if position is not None else 0)
        if name == "spec_tokens":
            return float(base.get("spec_tokens", spec_tokens))
        if name in CATEGORICAL_FEATURES:
            return encode_category(name, base.get(name))
        value = base.get(name)
        if value is None:
            return 0.0
        return float(value)

    for record in records:
        record_meta = dict(metadata)
        record_meta.update({k: v for k, v in (record.get("metadata") or {}).items() if v is not None})
        drafter_model = record_meta.get("drafter_model")
        verifier_model = record_meta.get("verifier_model")
        record_spec_tokens = record_meta.get("spec_tokens")
        try:
            record_spec_tokens = int(record_spec_tokens) if record_spec_tokens is not None else spec_tokens
        except (TypeError, ValueError):
            record_spec_tokens = spec_tokens

        for iteration in record.get("iterations", []):
            context_len = float(iteration.get("context_length_before", 0))
            accepted_flags: List[bool] = list(iteration.get("accepted_flags", []))
            draft_token_ids: List[int] = list(iteration.get("draft_token_ids", []))
            spec_len = min(record_spec_tokens, len(draft_token_ids)) if record_spec_tokens else len(draft_token_ids)
            accepted_count = sum(1 for flag in accepted_flags if flag)

            base_values: Dict[str, object] = {
                "context_length": context_len,
                "spec_tokens": record_spec_tokens or spec_tokens,
                "drafter_model": drafter_model,
                "verifier_model": verifier_model,
            }

            count_row = [feature_value(name, base_values, None) for name in COUNT_FEATURES]
            count_features.append(count_row)
            count_targets.append(accepted_count)

            for pos in range(min(spec_len, record_spec_tokens or spec_tokens)):
                if pos < len(accepted_flags):
                    flag = 1 if accepted_flags[pos] else 0
                    accept_row = [feature_value(name, base_values, pos) for name in ACCEPT_FEATURES]
                    accept_features.append(accept_row)
                    accept_targets.append(flag)

    if not count_features or not accept_features:
        raise ValueError("Insufficient data to train regressors.")

    X_count = np.array(count_features, dtype=np.float32)
    y_count = np.array(count_targets, dtype=np.float32)
    X_accept = np.array(accept_features, dtype=np.float32)
    y_accept = np.array(accept_targets, dtype=np.int32)
    categorical_maps = {feature: dict(mapping) for feature, mapping in category_maps.items()}

    return X_count, y_count, X_accept, y_accept, categorical_maps


def train_models(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
) -> Tuple[
    RandomForestRegressor,
    RandomForestClassifier,
    Dict[str, float],
]:
    # Accepted-count regressor
    Xc_train, Xc_test, yc_train, yc_test = train_test_split(
        X_count,
        y_count,
        test_size=test_size,
        random_state=random_state,
    )
    reg = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    reg.fit(Xc_train, yc_train)
    yc_pred = reg.predict(Xc_test)
    reg_mse = mean_squared_error(yc_test, yc_pred)
    reg_diagnostics = regression_metrics(yc_test, yc_pred)

    # Acceptance classifier
    Xa_train, Xa_test, ya_train, ya_test = train_test_split(
        X_accept,
        y_accept,
        test_size=test_size,
        random_state=random_state,
    )
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    clf.fit(Xa_train, ya_train)
    ya_proba = clf.predict_proba(Xa_test)
    ya_prob = positive_class_probabilities(clf, ya_proba)
    if len(getattr(clf, "classes_", [])) == 1:
        ya_pred = np.full(len(Xa_test), clf.classes_[0], dtype=np.int32)
    else:
        ya_pred = (ya_prob >= 0.5).astype(np.int32)
    clf_acc = accuracy_score(ya_test, ya_pred)
    clf_diagnostics = classification_metrics(ya_test, ya_pred, ya_prob, ece_bins=10)

    metrics = {
        "count_regression": reg_diagnostics,
        "accept_classification": clf_diagnostics,
        "count_mse": float(reg_mse),
        "accept_accuracy": float(clf_acc),
        "train_iterations": int(len(X_count)),
        "train_positions": int(len(X_accept)),
    }
    return reg, clf, metrics


def main() -> None:
    args = parse_args()
    records = load_records(args.details_jsonl)
    metadata = parse_metadata(args.metadata)

    X_count, y_count, X_accept, y_accept, categorical_maps = build_datasets(
        records,
        spec_tokens=args.spec_tokens,
        metadata=metadata,
    )

    reg_model, clf_model, metrics = train_models(
        X_count,
        y_count,
        X_accept,
        y_accept,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.random_state,
        test_size=args.test_size,
    )

    bundle = {
        "metadata": metadata,
        "spec_tokens": args.spec_tokens,
        "feature_columns": {
            "count": COUNT_FEATURES,
            "accept": ACCEPT_FEATURES,
        },
        "categorical_maps": categorical_maps,
        "regressor": reg_model,
        "classifier": clf_model,
        "metrics": metrics,
        "details_source": str(args.details_jsonl),
    }

    args.output_model.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(bundle, args.output_model)

    if args.print_report:
        print("Acceptance regressor training report")
        print(
            f"  iterations={metrics['train_iterations']} positions={metrics['train_positions']}"
        )
        count = metrics["count_regression"]
        print(
            "  Count regression: "
            f"MSE={count['mse']:.4f} MAE={count['mae']:.4f} "
            f"bias={count['bias']:.4f} p95_abs={count['p95_abs_error']:.4f}"
        )
        accept = metrics["accept_classification"]
        calib = accept.get("calibration")
        calib_str = f" ECE={calib['ece']:.4f}" if calib else ""
        print(
            "  Accept classifier: "
            f"accuracy={accept['accuracy']:.4f} precision={accept['precision']:.4f} "
            f"recall={accept['recall']:.4f} f1={accept['f1']:.4f} "
            f"brier={accept['brier']:.4f}{calib_str}"
        )


if __name__ == "__main__":
    main()
