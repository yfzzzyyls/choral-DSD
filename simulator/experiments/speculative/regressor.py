# Copyright 2025
# SPDX-License-Identifier: Apache-2.0
"""
Acceptance-rate regressor baseline inspired by VIDUR's profiling pipeline.

This script consumes per-iteration speculative decoding logs (generated by
``speculative.py`` with ``--details-jsonl``) and trains two models:

1. A regressor that predicts the number of speculative tokens accepted in each
   iteration (i.e., accepted_count).
2. A classifier that predicts the probability a token at position `pos` is
   accepted given the context length and position index.

Both models are RandomForest ensembles (matching VIDUR's default baseline).
The trained models plus lightweight metadata are serialized with joblib.

Example:
    python simulator/experiments/speculative/regressor.py \
        --details-jsonl results/simple_details.jsonl \
        --output-model acceptance/llama2_7b_vs_70b.joblib \
        --spec-tokens 4 \
        --metadata drafter=meta-llama/Llama-2-7b-hf \
        --metadata verifier=meta-llama/Llama-2-70b-hf \
        --print-report
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train acceptance-rate regressors from speculative logs.",
    )
    parser.add_argument(
        "--details-jsonl",
        type=Path,
        required=True,
        help="JSONL file produced by speculative.py (--details-jsonl).",
    )
    parser.add_argument(
        "--spec-tokens",
        type=int,
        required=True,
        help="Number of speculative positions (k).",
    )
    parser.add_argument(
        "--output-model",
        type=Path,
        required=True,
        help="Destination joblib file for the trained models.",
    )
    parser.add_argument(
        "--metadata",
        type=str,
        nargs="*",
        default=[],
        help="Optional key=value metadata to store alongside the model.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.2,
        help="Fraction of iterations/positions used for validation (default 0.2).",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=1337,
        help="Random seed for train/test split and model initialisation.",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Number of trees for RandomForest models (default 200).",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Maximum tree depth (default None => unrestricted).",
    )
    parser.add_argument(
        "--print-report",
        action="store_true",
        help="Print training/validation metrics after fitting.",
    )
    return parser.parse_args()


def load_records(path: Path) -> List[dict]:
    records: List[dict] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_number, line in enumerate(handle, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                payload = json.loads(line)
            except json.JSONDecodeError as exc:  # pragma: no cover
                raise ValueError(f"{path}:{line_number}: invalid JSON: {exc}") from exc
            records.append(payload)
    if not records:
        raise ValueError(f"No records found in {path}")
    return records


def parse_metadata(pairs: Iterable[str]) -> Dict[str, str]:
    metadata: Dict[str, str] = {}
    for pair in pairs:
        if "=" not in pair:
            raise ValueError(f"Metadata entry '{pair}' must be key=value")
        key, value = pair.split("=", 1)
        metadata[key] = value
    return metadata


def build_datasets(
    records: Iterable[dict],
    spec_tokens: int,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Returns:
        X_count (n_iterations, n_features),
        y_count (accepted_count),
        X_accept (n_position_attempts, n_features),
        y_accept (0/1 acceptance)
    """
    count_features: List[List[float]] = []
    count_targets: List[int] = []
    accept_features: List[List[float]] = []
    accept_targets: List[int] = []

    for record in records:
        prompt_idx = record.get("prompt_index", 0)
        for iteration in record.get("iterations", []):
            context_len = float(iteration.get("context_length_before", 0))
            accepted_flags: List[bool] = list(iteration.get("accepted_flags", []))
            draft_token_ids: List[int] = list(iteration.get("draft_token_ids", []))
            spec_len = min(spec_tokens, len(draft_token_ids))
            accepted_count = sum(1 for flag in accepted_flags if flag)

            count_features.append([context_len])
            count_targets.append(accepted_count)

            for pos in range(min(spec_len, spec_tokens)):
                if pos < len(accepted_flags):
                    flag = 1 if accepted_flags[pos] else 0
                    accept_features.append([context_len, float(pos)])
                    accept_targets.append(flag)

    if not count_features or not accept_features:
        raise ValueError("Insufficient data to train regressors.")

    X_count = np.array(count_features, dtype=np.float32)
    y_count = np.array(count_targets, dtype=np.float32)
    X_accept = np.array(accept_features, dtype=np.float32)
    y_accept = np.array(accept_targets, dtype=np.int32)

    return X_count, y_count, X_accept, y_accept


def train_models(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
) -> Tuple[
    RandomForestRegressor,
    RandomForestClassifier,
    Dict[str, float],
]:
    # Accepted-count regressor
    Xc_train, Xc_test, yc_train, yc_test = train_test_split(
        X_count,
        y_count,
        test_size=test_size,
        random_state=random_state,
    )
    reg = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    reg.fit(Xc_train, yc_train)
    yc_pred = reg.predict(Xc_test)
    reg_mse = mean_squared_error(yc_test, yc_pred)

    # Acceptance classifier
    Xa_train, Xa_test, ya_train, ya_test = train_test_split(
        X_accept,
        y_accept,
        test_size=test_size,
        random_state=random_state,
    )
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    clf.fit(Xa_train, ya_train)
    ya_pred = clf.predict(Xa_test)
    clf_acc = accuracy_score(ya_test, ya_pred)

    metrics = {
        "count_mse": float(reg_mse),
        "accept_accuracy": float(clf_acc),
        "train_iterations": int(len(X_count)),
        "train_positions": int(len(X_accept)),
    }
    return reg, clf, metrics


def main() -> None:
    args = parse_args()
    records = load_records(args.details_jsonl)
    metadata = parse_metadata(args.metadata)

    X_count, y_count, X_accept, y_accept = build_datasets(
        records,
        spec_tokens=args.spec_tokens,
    )

    reg_model, clf_model, metrics = train_models(
        X_count,
        y_count,
        X_accept,
        y_accept,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.random_state,
        test_size=args.test_size,
    )

    bundle = {
        "metadata": metadata,
        "spec_tokens": args.spec_tokens,
        "feature_columns": {
            "count": ["context_length"],
            "accept": ["context_length", "position"],
        },
        "regressor": reg_model,
        "classifier": clf_model,
        "metrics": metrics,
        "details_source": str(args.details_jsonl),
    }

    args.output_model.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(bundle, args.output_model)

    if args.print_report:
        print("Acceptance regressor training report")
        for key, value in metrics.items():
            print(f"  {key}: {value}")


if __name__ == "__main__":
    main()
