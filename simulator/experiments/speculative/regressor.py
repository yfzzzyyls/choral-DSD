# Copyright 2025
# SPDX-License-Identifier: Apache-2.0
"""
Acceptance-rate regressor baseline inspired by VIDUR's profiling pipeline.

This script consumes per-iteration speculative decoding logs (generated by
``speculative.py`` with ``--details-jsonl``) and trains two models:

1. A regressor that predicts the number of speculative tokens accepted in each
   iteration (i.e., accepted_count).
2. A classifier that predicts the probability a token at position `pos` is
   accepted given the context length and position index.

Both models are RandomForest ensembles (matching VIDUR's default baseline).
The trained models plus lightweight metadata are serialized with joblib.

Example:
    python simulator/experiments/speculative/regressor.py \
        --details-jsonl results/simple_details.jsonl \
        --output-model acceptance/llama2_7b_vs_70b.joblib \
        --spec-tokens 4 \
        --metadata drafter=meta-llama/Llama-2-7b-hf \
        --metadata verifier=meta-llama/Llama-2-70b-hf \
        --print-report
"""

from __future__ import annotations

import argparse
import json
from collections import defaultdict
from pathlib import Path
import sys
from typing import Dict, Iterable, List, Mapping, Optional, Sequence, Tuple

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import joblib
import numpy as np
from sklearn.ensemble import (
    HistGradientBoostingClassifier,
    HistGradientBoostingRegressor,
    RandomForestClassifier,
    RandomForestRegressor,
)
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split

from simulator.experiments.speculative.metrics import (
    classification_metrics,
    positive_class_probabilities,
    regression_metrics,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train acceptance-rate regressors from speculative logs.",
    )
    parser.add_argument(
        "--details-jsonl",
        type=Path,
        required=True,
        help="JSONL file produced by speculative.py (--details-jsonl).",
    )
    parser.add_argument(
        "--spec-tokens",
        type=int,
        required=True,
        help="Number of speculative positions (k).",
    )
    parser.add_argument(
        "--output-model",
        type=Path,
        required=True,
        help="Destination joblib file for the trained models.",
    )
    parser.add_argument(
        "--metadata",
        type=str,
        nargs="*",
        default=[],
        help="Optional key=value metadata to store alongside the model.",
    )
    parser.add_argument(
        "--algorithm",
        choices=["random_forest", "gbdt", "moe_gbdt"],
        default="random_forest",
        help="Regressor family to train (default: random_forest).",
    )
    parser.add_argument(
        "--min-group-samples",
        type=int,
        default=200,
        help="Minimum samples per expert when using moe_gbdt.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.2,
        help="Fraction of iterations/positions used for validation (default 0.2).",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=1337,
        help="Random seed for train/test split and model initialisation.",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Number of trees for RandomForest models (default 200).",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Maximum tree depth (default None => unrestricted).",
    )
    parser.add_argument(
        "--print-report",
        action="store_true",
        help="Print training/validation metrics after fitting.",
    )
    return parser.parse_args()


def load_records(path: Path) -> List[dict]:
    records: List[dict] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_number, line in enumerate(handle, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                payload = json.loads(line)
            except json.JSONDecodeError as exc:  # pragma: no cover
                raise ValueError(f"{path}:{line_number}: invalid JSON: {exc}") from exc
            records.append(payload)
    if not records:
        raise ValueError(f"No records found in {path}")
    return records


def parse_metadata(pairs: Iterable[str]) -> Dict[str, str]:
    metadata: Dict[str, str] = {}
    for pair in pairs:
        if "=" not in pair:
            raise ValueError(f"Metadata entry '{pair}' must be key=value")
        key, value = pair.split("=", 1)
        metadata[key] = value
    return metadata



COUNT_FEATURES = ["context_length", "spec_tokens", "drafter_model", "verifier_model"]
ACCEPT_FEATURES = COUNT_FEATURES + ["position"]
CATEGORICAL_FEATURES = {"drafter_model", "verifier_model"}
GROUP_FEATURES = ["drafter_model", "verifier_model", "spec_tokens"]
_UNKNOWN_CATEGORY = "__UNKNOWN__"

def build_datasets(
    records: Iterable[dict],
    spec_tokens: int,
    metadata: Optional[Mapping[str, str]] = None,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, Dict[str, int]], List[str], List[str], Dict[str, Dict[str, object]]]:
    """
    Returns:
        X_count (n_iterations, n_features),
        y_count (accepted_count),
        X_accept (n_position_attempts, n_features),
        y_accept (0/1 acceptance),
        categorical_maps (feature -> category -> index)
    """
    count_features: List[List[float]] = []
    count_targets: List[int] = []
    accept_features: List[List[float]] = []
    accept_targets: List[int] = []
    count_groups: List[str] = []
    accept_groups: List[str] = []
    group_metadata: Dict[str, Dict[str, object]] = {}
    category_maps: Dict[str, Dict[str, int]] = defaultdict(dict)
    metadata = metadata or {}

    def encode_category(feature: str, value: Optional[str]) -> float:
        key = str(value) if value is not None else _UNKNOWN_CATEGORY
        mapping = category_maps[feature]
        if key not in mapping:
            mapping[key] = len(mapping)
        return float(mapping[key])

    def feature_value(
        name: str,
        base: Dict[str, object],
        position: Optional[int],
    ) -> float:
        if name == "context_length":
            return float(base.get("context_length", 0.0))
        if name == "position":
            return float(position if position is not None else 0)
        if name == "spec_tokens":
            return float(base.get("spec_tokens", spec_tokens))
        if name in CATEGORICAL_FEATURES:
            return encode_category(name, base.get(name))
        value = base.get(name)
        if value is None:
            return 0.0
        return float(value)

    for record in records:
        record_meta = dict(metadata)
        record_meta.update({k: v for k, v in (record.get("metadata") or {}).items() if v is not None})
        drafter_model = record_meta.get("drafter_model")
        verifier_model = record_meta.get("verifier_model")
        record_spec_tokens = record_meta.get("spec_tokens")
        try:
            record_spec_tokens = int(record_spec_tokens) if record_spec_tokens is not None else spec_tokens
        except (TypeError, ValueError):
            record_spec_tokens = spec_tokens

        for iteration in record.get("iterations", []):
            context_len = float(iteration.get("context_length_before", 0))
            accepted_flags: List[bool] = list(iteration.get("accepted_flags", []))
            draft_token_ids: List[int] = list(iteration.get("draft_token_ids", []))
            spec_len = min(record_spec_tokens, len(draft_token_ids)) if record_spec_tokens else len(draft_token_ids)
            accepted_count = sum(1 for flag in accepted_flags if flag)

            base_values: Dict[str, object] = {
                "context_length": context_len,
                "spec_tokens": record_spec_tokens or spec_tokens,
                "drafter_model": drafter_model,
                "verifier_model": verifier_model,
            }
            group_key = "||".join([
                str(drafter_model or _UNKNOWN_CATEGORY),
                str(verifier_model or _UNKNOWN_CATEGORY),
                str(record_spec_tokens or spec_tokens),
            ])
            if group_key not in group_metadata:
                group_metadata[group_key] = {
                    "drafter_model": drafter_model,
                    "verifier_model": verifier_model,
                    "spec_tokens": record_spec_tokens or spec_tokens,
                }

            count_row = [feature_value(name, base_values, None) for name in COUNT_FEATURES]
            count_features.append(count_row)
            count_targets.append(accepted_count)
            count_groups.append(group_key)

            for pos in range(min(spec_len, record_spec_tokens or spec_tokens)):
                if pos < len(accepted_flags):
                    flag = 1 if accepted_flags[pos] else 0
                    accept_row = [feature_value(name, base_values, pos) for name in ACCEPT_FEATURES]
                    accept_features.append(accept_row)
                    accept_targets.append(flag)
                    accept_groups.append(group_key)

    if not count_features or not accept_features:
        raise ValueError("Insufficient data to train regressors.")

    X_count = np.array(count_features, dtype=np.float32)
    y_count = np.array(count_targets, dtype=np.float32)
    X_accept = np.array(accept_features, dtype=np.float32)
    y_accept = np.array(accept_targets, dtype=np.int32)
    categorical_maps = {feature: dict(mapping) for feature, mapping in category_maps.items()}

    return X_count, y_count, X_accept, y_accept, categorical_maps, count_groups, accept_groups, group_metadata


def train_models(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
    algorithm: str,
    count_groups: Sequence[str],
    accept_groups: Sequence[str],
    group_metadata: Mapping[str, Dict[str, object]],
    min_group_samples: int,
) -> Tuple[Dict[str, object], Dict[str, float]]:
    algo = algorithm.lower()
    if algo == "random_forest":
        reg, clf, metrics = _train_random_forest(
            X_count,
            y_count,
            X_accept,
            y_accept,
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            test_size=test_size,
        )
        model_bundle = {
            "algorithm": "random_forest",
            "regressor": reg,
            "classifier": clf,
        }
        return model_bundle, metrics
    if algo == "gbdt":
        reg, clf, metrics = _train_hist_gradient_boosting(
            X_count,
            y_count,
            X_accept,
            y_accept,
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            test_size=test_size,
        )
        model_bundle = {
            "algorithm": "gbdt",
            "regressor": reg,
            "classifier": clf,
        }
        return model_bundle, metrics
    if algo == "moe_gbdt":
        bundle, metrics = _train_moe_hist_gradient_boosting(
            X_count,
            y_count,
            X_accept,
            y_accept,
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            test_size=test_size,
            count_groups=count_groups,
            accept_groups=accept_groups,
            group_metadata=group_metadata,
            min_group_samples=min_group_samples,
        )
        return bundle, metrics
    raise ValueError(f"Unsupported algorithm '{algorithm}'")


def _train_random_forest(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
) -> Tuple[RandomForestRegressor, RandomForestClassifier, Dict[str, float]]:
    Xc_train, Xc_test, yc_train, yc_test = train_test_split(
        X_count,
        y_count,
        test_size=test_size,
        random_state=random_state,
    )
    reg = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    reg.fit(Xc_train, yc_train)
    yc_pred = reg.predict(Xc_test)
    reg_diagnostics = regression_metrics(yc_test, yc_pred)
    reg_mse = mean_squared_error(yc_test, yc_pred)

    Xa_train, Xa_test, ya_train, ya_test = train_test_split(
        X_accept,
        y_accept,
        test_size=test_size,
        random_state=random_state,
    )
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,
    )
    clf.fit(Xa_train, ya_train)
    ya_proba = clf.predict_proba(Xa_test)
    ya_prob = positive_class_probabilities(clf, ya_proba)
    if len(getattr(clf, "classes_", [])) == 1:
        ya_pred = np.full(len(Xa_test), clf.classes_[0], dtype=np.int32)
    else:
        ya_pred = (ya_prob >= 0.5).astype(np.int32)
    clf_acc = accuracy_score(ya_test, ya_pred)
    clf_diagnostics = classification_metrics(ya_test, ya_pred, ya_prob, ece_bins=10)

    metrics = {
        "count_regression": reg_diagnostics,
        "accept_classification": clf_diagnostics,
        "count_mse": float(reg_mse),
        "accept_accuracy": float(clf_acc),
        "train_iterations": int(len(X_count)),
        "train_positions": int(len(X_accept)),
    }
    return reg, clf, metrics


def _train_hist_gradient_boosting(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
) -> Tuple[HistGradientBoostingRegressor, HistGradientBoostingClassifier, Dict[str, float]]:
    Xc_train, Xc_test, yc_train, yc_test = train_test_split(
        X_count,
        y_count,
        test_size=test_size,
        random_state=random_state,
    )
    reg = HistGradientBoostingRegressor(
        max_iter=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
    )
    reg.fit(Xc_train, yc_train)
    yc_pred = reg.predict(Xc_test)
    reg_diagnostics = regression_metrics(yc_test, yc_pred)
    reg_mse = mean_squared_error(yc_test, yc_pred)

    Xa_train, Xa_test, ya_train, ya_test = train_test_split(
        X_accept,
        y_accept,
        test_size=test_size,
        random_state=random_state,
    )
    clf = HistGradientBoostingClassifier(
        max_iter=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
    )
    clf.fit(Xa_train, ya_train)
    ya_proba = clf.predict_proba(Xa_test)
    ya_prob = positive_class_probabilities(clf, ya_proba)
    if len(getattr(clf, "classes_", [])) == 1:
        ya_pred = np.full(len(Xa_test), clf.classes_[0], dtype=np.int32)
    else:
        ya_pred = (ya_prob >= 0.5).astype(np.int32)
    clf_acc = accuracy_score(ya_test, ya_pred)
    clf_diagnostics = classification_metrics(ya_test, ya_pred, ya_prob, ece_bins=10)

    metrics = {
        "count_regression": reg_diagnostics,
        "accept_classification": clf_diagnostics,
        "count_mse": float(reg_mse),
        "accept_accuracy": float(clf_acc),
        "train_iterations": int(len(X_count)),
        "train_positions": int(len(X_accept)),
    }
    return reg, clf, metrics


def _fit_hist_gradient_boosting_models(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
) -> Tuple[HistGradientBoostingRegressor, HistGradientBoostingClassifier]:
    reg = HistGradientBoostingRegressor(
        max_iter=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
    )
    reg.fit(X_count, y_count)
    clf = HistGradientBoostingClassifier(
        max_iter=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
    )
    clf.fit(X_accept, y_accept)
    return reg, clf


def _train_moe_hist_gradient_boosting(
    X_count: np.ndarray,
    y_count: np.ndarray,
    X_accept: np.ndarray,
    y_accept: np.ndarray,
    *,
    n_estimators: int,
    max_depth: Optional[int],
    random_state: int,
    test_size: float,
    count_groups: Sequence[str],
    accept_groups: Sequence[str],
    group_metadata: Mapping[str, Dict[str, object]],
    min_group_samples: int,
) -> Tuple[Dict[str, object], Dict[str, float]]:
    reg, clf, metrics = _train_hist_gradient_boosting(
        X_count,
        y_count,
        X_accept,
        y_accept,
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        test_size=test_size,
    )

    count_groups_arr = np.array(count_groups)
    accept_groups_arr = np.array(accept_groups)
    experts: Dict[str, Dict[str, object]] = {}
    unique_keys = sorted(set(count_groups_arr))
    for key in unique_keys:
        count_idx = np.where(count_groups_arr == key)[0]
        accept_idx = np.where(accept_groups_arr == key)[0]
        if len(count_idx) < min_group_samples or len(accept_idx) < min_group_samples:
            continue
        expert_reg, expert_clf = _fit_hist_gradient_boosting_models(
            X_count[count_idx],
            y_count[count_idx],
            X_accept[accept_idx],
            y_accept[accept_idx],
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state + (hash(key) % 9973),
        )
        experts[key] = {
            "regressor": expert_reg,
            "classifier": expert_clf,
        }

    metrics["expert_count"] = len(experts)
    model_bundle = {
        "algorithm": "moe_gbdt",
        "regressor": reg,
        "classifier": clf,
        "experts": experts,
        "group_features": GROUP_FEATURES,
        "group_info": dict(group_metadata),
    }
    return model_bundle, metrics


def main() -> None:
    args = parse_args()
    records = load_records(args.details_jsonl)
    metadata = parse_metadata(args.metadata)

    (
        X_count,
        y_count,
        X_accept,
        y_accept,
        categorical_maps,
        count_groups,
        accept_groups,
        group_metadata,
    ) = build_datasets(
        records,
        spec_tokens=args.spec_tokens,
        metadata=metadata,
    )

    model_bundle, metrics = train_models(
        X_count,
        y_count,
        X_accept,
        y_accept,
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.random_state,
        test_size=args.test_size,
        algorithm=args.algorithm,
        count_groups=count_groups,
        accept_groups=accept_groups,
        group_metadata=group_metadata,
        min_group_samples=args.min_group_samples,
    )

    bundle = {
        "metadata": metadata,
        "spec_tokens": args.spec_tokens,
        "feature_columns": {
            "count": COUNT_FEATURES,
            "accept": ACCEPT_FEATURES,
        },
        "categorical_maps": categorical_maps,
        "metrics": metrics,
        "details_source": str(args.details_jsonl),
    }
    bundle.update(model_bundle)

    args.output_model.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(bundle, args.output_model)

    if args.print_report:
        print("Acceptance regressor training report")
        print(
            f"  iterations={metrics['train_iterations']} positions={metrics['train_positions']}"
        )
        count = metrics["count_regression"]
        print(
            "  Count regression: "
            f"MSE={count['mse']:.4f} MAE={count['mae']:.4f} "
            f"bias={count['bias']:.4f} p95_abs={count['p95_abs_error']:.4f}"
        )
        accept = metrics["accept_classification"]
        calib = accept.get("calibration")
        calib_str = f" ECE={calib['ece']:.4f}" if calib else ""
        print(
            "  Accept classifier: "
            f"accuracy={accept['accuracy']:.4f} precision={accept['precision']:.4f} "
            f"recall={accept['recall']:.4f} f1={accept['f1']:.4f} "
            f"brier={accept['brier']:.4f}{calib_str}"
        )


if __name__ == "__main__":
    main()
